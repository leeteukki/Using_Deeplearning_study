{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한꺼번에 다운받는 데 필요한 처리 내용\n",
    "\n",
    "지금까지 BeautifulSoup와  CSS 선택자의 사용법을 살펴봄. 하지만 이것만으로는 링크에 있는 것을 한꺼번에 다운받을 수 없음.\n",
    "\n",
    "일단 a 태그의 링크 대상이 상대 경로일 수 있음. 그래서 링크 대상이 HTML일 경우, 해당 HTML의 내용에 추가적인 처리를 해야함.\n",
    "그리고 링크를 재귀적으로 다운받아야 함. 이번 절에서는 링크에 있는 것을 한꺼번에 다운받는 기법을 소개하겠음.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상대 경로를 전개하는 방법\n",
    "\n",
    "그럼 일단 첫 번째 문제부터 살펴보겠음.\n",
    "일단 a 태그의 href 속성에 링크 대상이 \"../img/hoge.png\" 처럼 상대 경로로 적혀 있다고 합시다. a 태그가 상대 경로로 주어졌을 때 대상에 있는 것을 다운받으려면 상대 경로를 절대 경로로 변환해야함. \n",
    "\n",
    "상대 경로를 전개할 때는 urllib.parser.urljoin()을 사용함. 실제로 프로그램을 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/html/b.html\n",
      "http://example.com/html/sub/c.html\n",
      "http://example.com/index.html\n",
      "http://example.com/img/hoge.png\n",
      "http://example.com/css/hoge.css\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"http://example.com/html/a.html\"\n",
    "\n",
    "print(urljoin(base, \"b.html\"))\n",
    "print(urljoin(base, \"sub/c.html\"))\n",
    "print(urljoin(base, \"../index.html\"))\n",
    "print(urljoin(base, \"../img/hoge.png\"))\n",
    "print(urljoin(base, \"../css/hoge.css\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과를 보면 기본 URL을 기반으로 상대 경로를 절대 경로로 변환한다는 것을 알 수 있음. 이처럼 상대 경로를 절대 경로로 변환하는 urljoin() 함수의 사용법을 살펴봅시다.\n",
    "\n",
    "[서식] urllib.parse.urljoin()의 사용법\n",
    "urljoin(base, path)\n",
    "\n",
    "이 함수는 첫 번째 매개뱐수로 기본 URL, 두 번째 매개변수로 상대 경로를 지정함.\n",
    "\n",
    "만약 상대 경로(path 매개변수)가 http:// 등으로 시작한다면 기본 URL(base 매개뱐수)를 무시하고, 번째 매개변수에 지정한 URL 리런함.\n",
    "예제 코드를 동작시켜보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/hoge.html\n",
      "http://otherExample.com/wiki\n",
      "http://anotherExample.org/test\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"http://example.com/html/a.html\"\n",
    "\n",
    "print( urljoin(base, \"/hoge.html\"))\n",
    "print( urljoin(base, \"http://otherExample.com/wiki\"))\n",
    "print( urljoin(base, \"//anotherExample.org/test\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 urljoin() 함수를 사용하면 a 태그의 href 속성에 지정돼 있는 경로를 절대 경로로 쉽게 변환할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모든 페이지를 한꺼번에 다운받는 프로그램\n",
    "\n",
    "일단 이번 절에서는 웹에 있는 파이썬 문서 중에서 library 폴더 아래에 있는 모든 것을 다운받아보겠음. \n",
    "\n",
    "프로그램을 실행하면 사이트 내부의 파일 또는 HTML 등을 모두 다운로드 받음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir =  ./docs.python.org/3.5/library\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'makdirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-38f1d5c05c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# URL에 있는 모든 것 다운로드하기 -- 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://docs.python.org/3.5/library/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0manalyze_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-38f1d5c05c9b>\u001b[0m in \u001b[0;36manalyze_html\u001b[0;34m(url, root_url)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# HTML을 분석하고 다운받는 함수 -- 8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalyze_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0msavepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msavepath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msavepath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mproc_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;31m# 이미 처리되었다면 실행하지 않음 -- 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-38f1d5c05c9b>\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mkdir = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mmakdirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# 파일 다운로드 받기 -- 6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'makdirs' is not defined"
     ]
    }
   ],
   "source": [
    "# 파이썬 메뉴얼을 재귀적으로 다운받는 프로그램\n",
    "# 모듈 읽어들이기 -- 1\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "from urllib.parse import *\n",
    "from os import makedirs\n",
    "import os.path, time, re\n",
    "\n",
    "# 이미 처리한 파일이지 확인하기 위한 변수 -- 2\n",
    "proc_files = {}\n",
    "\n",
    "# HTML 내부에 있는 링크를 추출하는 함수 -- 3\n",
    "def enum_links(html, base):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.select(\"link[rel = 'stylesheet']\") # CSS\n",
    "    links += soup.select(\"a[href]\") # 링크\n",
    "    result = []\n",
    "    \n",
    "    # href 속성을 추출하고, 링크를 절대 경로로 변환 -- 4\n",
    "    for a in links:\n",
    "        href = a.attrs['href']\n",
    "        url = urljoin(base, href)\n",
    "        result.append(url)\n",
    "    return result\n",
    "\n",
    "# 파일을 다운받고 저장하는 함수 -- 5\n",
    "def download_file(url):\n",
    "    o = urlparse(url)\n",
    "    savepath = \"./\" + o.netloc + o.path\n",
    "    if re.search(r\"/$\", savepath): # 폴더라면 index.html\n",
    "        savepath += \"index.html\"\n",
    "    savedir = os.path.dirname(savepath)\n",
    "    #  모두 다운로드 됐는지 확인하기 \n",
    "    if not os.path.exists(savedir):\n",
    "        print(\"mkdir = \", savedir)\n",
    "        makdirs(savedir)\n",
    "    # 파일 다운로드 받기 -- 6\n",
    "    try :\n",
    "        print(\"download = \", url)\n",
    "        urlretrieve(url, savepath)\n",
    "        time.sleep(1) # 1초 휴식 -- 7\n",
    "        return savepath\n",
    "    except:\n",
    "        print(\"download_fail : \", url)\n",
    "        return None\n",
    "# HTML을 분석하고 다운받는 함수 -- 8\n",
    "def analyze_html(url, root_url):\n",
    "    savepath = download_file(url)\n",
    "    if savepath is None: return\n",
    "    if savepath is proc_files: return # 이미 처리되었다면 실행하지 않음 -- 9\n",
    "    proc_files[savepath] = True\n",
    "    print(\"analyze_html = \", url)\n",
    "    \n",
    "    for link_url in links:\n",
    "        # 링크가 루트 이외의 경로를 나타낸다면 무시 -- 11\n",
    "        if link_url.find(root_url) != 0:\n",
    "            if not re.search(r\".css$\", link_url) : continue\n",
    "        # HTML 이라면\n",
    "        if re.search(r\".(html|htm)$\", link_url):\n",
    "            # 재귀적으로 HTML 파일 분석하기\n",
    "            analyze_html(link_url, root_url)\n",
    "            continue\n",
    "        # 기타 파일\n",
    "        download_file(link_url)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    # URL에 있는 모든 것 다운로드하기 -- 12\n",
    "    url = \"https://docs.python.org/3.5/library/\"\n",
    "    analyze_html(url, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
