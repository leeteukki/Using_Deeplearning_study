{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Beautifulsoup로 스크레이핑하기\n",
    "\n",
    "스크레이핑이란 웹 사이트에서 데이터를 추출하고, 원하는 정보를 추출하는 것임. 최근에는 인터넷에 데이터가 너무 많으므로 스크레이핑을 잘 활용하는 것이 중요함.\n",
    "\n",
    "파이썬으로 스크레이핑할 때 뺴놓을 수 없는 라이브러리가 바로 \"BeautifulSoup\"임. 이 라이브러리를 이용하면 간단하게 HTML과 XML에서 정보를 추출할 수 있음.\n",
    "\n",
    "최근 스크레이핑 라이브러리는 다운로드부터 HTML 분석까지 모두 해주는 경우가 많은데, BeautifulSoup는 어디까지나 HTML과 XML을 분석해주는 라이브러리임. BeautifulSoup 자체에는 다운로드 기능이 없으므로 주의해줘야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 설치하기\n",
    "\n",
    "파이썬 라이브러리를 설치할 때는 pip 명령어를 사용함. pip란 파이썬 패키지 관리 시스템임.\n",
    "\n",
    "파이썬 패키지는 Python Package Index(PyPI)에서 확인할 수 있음. pip을 이용하면 PyPi에 있는 패키지를 명령어 한 줄로 설치할 수 있음.\n",
    "\n",
    "pip로 BeautifulSoup를 설치할 때는 다음과 같은 명령어를 실행함\n",
    " \n",
    " $ pip3 install beautifulsoup4\n",
    " \n",
    " 참고로 pip3는 pip의 파이썬3 버전임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup 기본 사용법\n",
    "\n",
    "일단은 Beautifulsoup의 기본적인 사용법을 확인해봅시다. 다음 프로그램은 BeautifulSoup를 이용해 분석하는 간단한 예제임. 웹 사이트로부터 HTML을 가져와서 사용하는 것이 아니라 HTML을 문자열로 만들어 사용하고 있음. 그리고 문자열 분석을 완료하면 결과를 출력함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 =  스크레이핑란? \n",
      "p =  웹 페이지를 분석하는 것\n",
      "p =  원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 읽어 들이기 -- 1\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석하고 싶은 HTML -- 2\n",
    "html = \"\"\"\n",
    "<html><body>\n",
    "    <h1> 스크레이핑란? </h1>\n",
    "    <p> 웹 페이지를 분석하는 것</p>\n",
    "    <p> 원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 -- 3\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 원하는 부분 추출하기 -- 4\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "# 요소의 글자를 출력하기 -- 5\n",
    "print(\"h1 = \" + h1.string)\n",
    "print(\"p = \" + p1.string)\n",
    "print(\"p = \" + p2.string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로그램 (1)에서는 Beautifulsoup 라이브러리를 읽어 들임. (2)에서는 분석 대상 HTML을 지정함.\n",
    "프로그램의 (3)에서는 BeautifulSoup 인스턴스를 생성. 이때 첫 번째 매개변수에 HTML을 지정함. 그리고 두 번째 매개변수에는 분석할 분석기(parser)의 종류를 지정함. HTML을 분석할 때는 \"html.parser\"라고 지정함.\n",
    "\n",
    "프로그램의 (4)에서 원하는 부분을 추출함. 정상적으로 분석됐다면 HTML의 구조처럼 루트 요소인 <html>에서 마침표(.)를 사용해 값에 접근할 수 있음. 코드에서는 \"soup.html.body.h1\" 이라고 적었는데, HTML, body, h1 에 있는 요소에 접근한 것임. 프로그램의 (5)에서는 string 속성에 접근해서 요소의 글자 부분을 추출함.\n",
    "    \n",
    "분석할 때 HTML 내부에는 <p> 태그가 2개 있는데 soup.html.body.p라고 접근하면 앞쪽에 있는 <p> 태그를 추출하게 됨. 이때 첫 번째의 next_sibling에서는 </p> 뒤에 있는 줄바꿈 또는 공백이 추출됨. 따라서 next_sibling을 한 번 더 사용해 2번째 <p> 태그를 추출함.\n",
    "    \n",
    "HTML의 구조를 알고 있다면 쉽게 원하는 요소를 추출할 수 있음. 하지만 루투부터 \"html.body..\" 형태로 HTML 구조를 하나하나 적어 나가는 것은 조금 귀찮고 복잡함. 따라서 이어서 간단하게 요소를 찾아내는 방법을 소개하겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# id로 요소를 찾는 방법\n",
    "\n",
    "BeautifulSoup는 루트부터 하나하나 요소를 찾는 방법 말고도 id속성을 지정해서 요소를 찾는 find()메서드라는 메서드를 제공함. 그럼 곧바로 코드를 살펴보겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title = 스크래핑이란? \n",
      "#body = 원하는 부분을 추출하는 것 \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <h1 id= \"title\"> 스크래핑이란? </h>\n",
    "        <p> 웹 페이지를 분석하는 것 </p>\n",
    "        <p id = \"body\"> 원하는 부분을 추출하는 것 </p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 -- 1\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find() 메서드로 원하는 부분 추출하기 -- 2\n",
    "title = soup.find(id = \"title\")\n",
    "body  = soup.find(id = \"body\")\n",
    "\n",
    "# 텍스트 부분 출력하기\n",
    "print(\"#title =\" + title.string)\n",
    "print(\"#body =\" + body.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로그램의 (1)에서는 BeautifulSoup 인스턴스를 생성함. 첫 번째 매개변수에 분석하고 싶은 HTML을 지정함.\n",
    "(2)에서는 id를 지정해 요소를 추출함. find() 메서드에 \"id=<값>\" 형태로 매개변수를 지정해 요소를 검색함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 여러 개의 요소 추출하기 -  find_all() 메서드\n",
    "\n",
    "참고로 여로 개의 태그를 한 번에 추출하고 싶을 때는 find_all() 메서드를 사용함. 다음 코드는 HTML 내부에 있는 여러 개의 \"a\" 태그를 추출하는 프로그램임. \"a\" 태그는 하이퍼링크 태그이므로, 링크 대상은 href 속성으로 지정하고 링크를 설명하는 텍스트는 태그 내부에 입력. 다음 코드는 설명 글자와 링크 대상 URL을 추출하고 출력하는 예임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " naver  > http://www.naver.com\n",
      " daum  > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <ul>\n",
    "            <li><a href = \"http://www.naver.com\"> naver </a></li>\n",
    "            <li><a href = \"http://www.daum.net\"> daum </a></li>\n",
    "        </ul>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 -- 1\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# find_all() 메서드로 추출하기 -- 2\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "# 링크 목록 출력하기 -- 3\n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text, \">\", href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로그램 (1)에서는 HTML을 지정해 beautifulSoup 인스턴스를 생성함. 프로그램의 (2)에서는 find_all() 메서드를 사용해 a 테그를 추출함.\n",
    "\n",
    "프로그램의 (3)에서는 추출한 모든 요소를 for 구문으로 반복 처리함. 링크의 href 속성은 attrs['href'] 처럼 attrs 속성에서 추출함. 또한 내부의 설명 텍스는 string 속성으로 추출함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOM 요소의 속성에 대해\n",
    "\n",
    "그럼 다시 DOM 요소의 속성을 추출하는 방법을 확인해봅시다. 파이썬의 대화형 실제 환경인 REPL을 사용해 동작을 확인해보겠습니다.\n",
    "REPL을 실행하려면 명령줄에  \"python3\"라고 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-f88ca815835f>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-f88ca815835f>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    \"html.parser\")\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pyhon3\n",
    ">>> # 코드를 쉽게 볼 수 있게 줄바꿈했습니다. 실제 REPL은 따로 줄바꿈되지 않습니다.\n",
    ">>> from bs4 import BeautifulSoup\n",
    "... \"<p><a href = 'a.href'>test</a></p>\",\n",
    "... \"html.parser\")\n",
    "\n",
    ">>> # 분석이 제대로 됐는지 확인하기 \n",
    ">>> soup.prettify()\n",
    "'<p>\\n <a href = \"a.html\">\\n test\\n </a>\\n</p>'\n",
    "\n",
    ">>> # attrs 속성의 자료형 확인\n",
    ">>> type(a.attrs)\n",
    "<class 'dict'>\n",
    "\n",
    ">>> # href 속성이 있는지 확인\n",
    ">>> 'href' in a.attrs\n",
    "True\n",
    "\n",
    ">>> # href 속성값 확인\n",
    ">>> a['href']\n",
    "'a_html'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# urlopen() 과 Beautifulsoup 조합하기\n",
    "\n",
    "BeautifulSoup 인스턴스를 생성하는 방법을 배웠습니다. 지금까지 살펴본 예제처럼 HTML 문자열을 지정할 수도 있지만 open()함수 또는 urllib.request.urlopen() 함수의 리턴 값을 지정해도됨.\n",
    "그럼 urlopen()을 사용해 \"기상청 RSS\"에서 특정 내용을 추출해보겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (강수) 15일(금)과 16일(토)은 전국에 비가 오겠고. 강원도는 17일(일)까지 이어지겠습니다.<br />○ (기온) 20일(수)까지 낮 기온은 어제(16~22도)보다 높은 20~28도의 분포가 되겠습니다.<br />          한편, 이번 예보기간에는 내륙을 중심으로 25도 이상 오르는 곳이 많아 조금 덥겠으나, <br />          16일(토)은 비로 인해 낮 기온이 오르지 못해 20~23도의 분포가 되겠습니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "# urlopen()으로 데이터 가져오기\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeaufulSoup으로 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출하기\n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "굉장히 간단한 프로그램으로서 기상청 RSS에서 XML 데이터를 추출하고 XML의 내용을 출력함\n",
    "\n",
    "프로그램 (1)에서는 urlopen()으로 URL을 엽니다. 그리고 프로그램의(2)에서 BeautifulSoup로 분석함. \n",
    "(3)에서 원하는 태그를 추출하고, 결과를 출력함\n",
    "참고로 이번 코드에서는 프로그램을 쉽게 볼 수 있게 urlopen()을 실행할 때 with 구문을 사용하지 않았음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS 선택자 사용하기\n",
    "\n",
    "BeautifulSoup는 자바스크립트 라이브러리인 jQuery처럼 CSS 선택자를 지정해서 원하는 요소를 추출하는 기능도 제공함.\n",
    "다음은 HTML에서 h1 태그와 li 태그를 추출해 출력하는 코드임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 =   위키북스 도서 \n",
      "li =   유니티 게임 이펙트 입문 \n",
      "li =   스위프트로 시작하는 아이폰 앱 개발 교과서 \n",
      "li =   모던 웹사이트 디자인의 정석 \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 분석 대상 HTML -- 1\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <body>\n",
    "        <div id = \"meigen\">\n",
    "        <h1> 위키북스 도서 </h1>\n",
    "            <ul class = \"items\">\n",
    "                <li> 유니티 게임 이펙트 입문 </li>\n",
    "                <li> 스위프트로 시작하는 아이폰 앱 개발 교과서 </li>\n",
    "                <li> 모던 웹사이트 디자인의 정석 </li>\n",
    "            </ul>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석하기 -- 2\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# 필요한 부분을 CSS 쿼리로 추출하기\n",
    "# 타이틀 부분 추출하기 -- 3\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(\"h1 = \", h1)\n",
    "\n",
    "# 목록 부분 추출하기 -- 4\n",
    "li_list = soup.select(\"div#meigen > ul.items > li\")\n",
    "for li in li_list:\n",
    "    print(\"li = \", li.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "프로그램 (1)에서는 분석 대상 HTML을 지정함.\n",
    "(2)에서는 BeautifulSoup 인스턴스를 생서함. 이때 내부적으로 분석 처리가 이루어짐.\n",
    "\n",
    "프로그램의 (3)에서는 select_one() 메서드를 사용해 h1 태그를 추출함. 이 예에서는 h1 태그가 하나밖에 없으므로 h1이라고만 지정해도 상관없음. 실제로 웹 사이트에서 추출한 HTML에서 원하는 요소를 선택하려면 CSS 선택자를 조금 더 자세히 지정해야함.\n",
    "\n",
    "그리고 프로그램의 (4)에서는 select() 메서드를 사용해 여러개의 h1 태그를 추출함. 추출한 요소는 리스트 자료형이므로 for 구문을 사용해 하나씩 확인함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 금융에서 환율 추출하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw = 1,221.00\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "# HTML 가져오기\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# HTML 분석하기\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출하기 -- 1\n",
    "price = soup.select_one(\"div.head_info > span.value\").string\n",
    "print(\"usd/krw =\", price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
